{"nbformat_minor": 0, "cells": [{"execution_count": 7, "cell_type": "code", "source": "from stemming.porter2 import stem\nimport os, sys\nimport math\nimport numpy as np\nimport scipy\nfrom scipy import sparse\nfrom scipy.sparse import hstack, vstack\nlib_path = os.path.abspath(os.path.join('spamEmailClassifier'))\nsys.path.append(lib_path)\nfrom Preprocesser import * \nfrom NaiveBayes import *\nfrom svm import *\n\n# spamDir = '\"D:\\\\projects\\\\spamEmailClassifier\\\\spamDataset\"\n# nonspamDir = \"D:\\\\projects\\\\spamEmailClassifier\\\\nonspamDataset\"\nspamDir = 'spamDataset'\nnonspamDir = 'nonspamDataset'", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# My Naive Bayes Classifier", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "trainingSetSpamFileList, testSetSpamFileList, trainingSetNonSpamFileList, testSetNonSpamFileList = getTrainingTestSet(spamDir, nonspamDir)\ntrainingSpamTokenList, testSpamTokenList, trainingNonSpamTokenList, testNonSpamTokenList = getNormalizedEmailList(trainingSetSpamFileList, testSetSpamFileList, trainingSetNonSpamFileList, testSetNonSpamFileList)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 9, "cell_type": "code", "source": "wordList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability =  getSpamWordList(trainingSpamTokenList,trainingNonSpamTokenList, 4, 9, 3, 99)\nprint len(wordList)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1287\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Prediction ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "p,x,y = predict(trainingSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\ns =  sum(p) / float(len(p))\nprint \"training set spam: \", s\np,x,y = predict(trainingNonSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nns =  1 - sum(p) / float(len(p))\nprint \"training set non-spam: \", ns\nprint \"training set overall: \", s * spamProbability + ns * (1 - spamProbability)\n\nprint \"\"\np,x,y = predict(testSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\ns =  sum(p) / float(len(p))\nprint \"test set spam: \", s\np,x,y = predict(testNonSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nns =  1 - sum(p) / float(len(p))\nprint \"test set non-spam: \", ns\nprint \"test set overall: \", s * spamProbability + ns * (1 - spamProbability)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training set spam:  0.934895833333\ntraining set non-spam:  0.997407983411\ntraining set overall:  0.987726049309\n\ntest set spam:  0.933471933472\ntest set non-spam:  0.996268656716\ntest set overall:  0.986542647684\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 11, "cell_type": "code", "source": "myEmails = [\"I like ml\", \"here is a good offer to make millions dollars from littel investment\", \"what are you doing\"]\nx = map(lambda x : getTokensFromStr(x), myEmails)\np,xs,yds = predict(x, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nprint p", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[False, True, False]\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Skkit-learn Naive Bayes", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "from sklearn.feature_extraction.text import CountVectorizer\ndef readFileListAndNormalize(fileList):\n    res = []\n    for i in fileList:\n        with open(i, 'r') as file :\n            data=file.read()\n            res = res + [data]\n    return res", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 13, "cell_type": "code", "source": "trainingSpamEmails = readFileListAndNormalize(trainingSetSpamFileList)\ntestSpamEmails = readFileListAndNormalize(testSetSpamFileList)\ntrainingNonSpamEmails = readFileListAndNormalize(trainingSetNonSpamFileList)\ntestNonSpamEmails = readFileListAndNormalize(testSetNonSpamFileList)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 14, "cell_type": "code", "source": "count_vect = CountVectorizer()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 15, "cell_type": "code", "source": "X_train_counts = count_vect.fit_transform(trainingSpamEmails + trainingNonSpamEmails)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 16, "cell_type": "code", "source": "print X_train_counts[0].shape", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(1, 49300)\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 17, "cell_type": "code", "source": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 18, "cell_type": "code", "source": "target = map (lambda x : 1, trainingSpamEmails)\ntarget += map(lambda x : 0, trainingNonSpamEmails)\n\ntestTarget = map (lambda x : 1, testSpamEmails)\ntestTarget += map(lambda x : 0, testNonSpamEmails)\nprint len(target), len(trainingSpamEmails) + len(trainingNonSpamEmails)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "2313 2313\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 19, "cell_type": "code", "source": "from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB().fit(X_train_tfidf, target)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### predict new emails", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "X_train_spam_count =  count_vect.transform(trainingSpamEmails)\nX_train_nonspam_count =  count_vect.transform(trainingNonSpamEmails)\nX_train_spam_tfidf =  tfidf_transformer.transform(X_train_spam_count)\nX_train_nonspam_tfidf =  tfidf_transformer.transform(X_train_nonspam_count)\n\nX_test_spam_count =  count_vect.transform(testSpamEmails)\nX_test_nonspam_count =  count_vect.transform(testNonSpamEmails)\nX_test_spam_tfidf =  tfidf_transformer.transform(X_test_spam_count)\nX_test_nonspam_tfidf =  tfidf_transformer.transform(X_test_nonspam_count)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 21, "cell_type": "code", "source": "X_train_spam_predicted = clf.predict(X_train_spam_tfidf) \nprint \"training spam set: \", float(np.sum(X_train_spam_predicted)) / len(X_train_spam_predicted)\n\nX_train_nonspam_predicted = clf.predict(X_train_nonspam_tfidf) \nprint \"training nonspam set: \", float(np.sum(X_train_nonspam_predicted)) / len(X_train_nonspam_predicted)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training spam set:  0.619791666667\ntraining nonspam set:  0.00259201658891\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 22, "cell_type": "code", "source": "X_test_spam_predicted = clf.predict(X_test_spam_tfidf) \nprint \"test spam set: \", float(np.sum(X_test_spam_predicted)) / len(X_test_spam_predicted)\n\nX_test_nonspam_predicted = clf.predict(X_test_nonspam_tfidf) \nprint \"test nonspam set: \", float(np.sum(X_test_nonspam_predicted)) / len(X_test_nonspam_predicted)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "test spam set:  0.602910602911\ntest nonspam set:  0.00248756218905\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Instead of using tfidf,  my wordList generate better result.  SHOULD FIGURE OUT tfidf", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "trainingSpamNormalizedEmails = map(lambda x : ' '.join(x), trainingSpamTokenList)\ntestSpamNormalizedEmails = map(lambda x : ' '.join(x), testSpamTokenList)\ntrainingNonSpamNormalizedEmails = map(lambda x : ' '.join(x), trainingNonSpamTokenList)\ntestNonSpamNormalizedEmails = map(lambda x : ' '.join(x), testNonSpamTokenList)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 24, "cell_type": "code", "source": "count_vect_normalized = CountVectorizer()\ncount_vect_normalized.vocabulary = wordList\nX_train_counts_normalized = count_vect_normalized.transform(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails)\nclf_normalized = BernoulliNB().fit(X_train_counts_normalized, target)\n\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 25, "cell_type": "code", "source": "X_train_spam_normalized_predicted = clf_normalized.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails)) \nprint \"training spam set: \", float(np.sum(X_train_spam_normalized_predicted)) / len(X_train_spam_normalized_predicted)\n\nX_train_nonspam_normalized__predicted = clf_normalized.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint \"training nonspam set: \", 1- float(np.sum(X_train_nonspam_normalized__predicted)) / len(X_train_nonspam_normalized__predicted)\n\n\npredict = clf_normalized.predict(count_vect_normalized.transform(testSpamNormalizedEmails)) \nprint \"test spam set: \", float(np.sum(predict)) / len(predict)\n\npredict = clf_normalized.predict(count_vect_normalized.transform(testNonSpamNormalizedEmails))\nprint \"test nonspam set: \", 1- float(np.sum(predict)) / len(predict)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training spam set:  0.908854166667\ntraining nonspam set:  0.998963193364\ntest spam set:  0.906444906445\ntest nonspam set:  0.999170812604\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### use pipeline", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "from sklearn.pipeline import Pipeline\ntext_clf = Pipeline([('vect', CountVectorizer()),\n     ('tfidf', TfidfTransformer()),\n     ('clf', MultinomialNB()),\n])\ntext_clf = text_clf.fit(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails, target)  ", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 27, "cell_type": "code", "source": "predicted = text_clf.predict(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails)\nprint predicted\nnp.mean(predicted == target)    ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[0 1 0 ..., 0 0 0]\n"}, {"execution_count": 27, "output_type": "execute_result", "data": {"text/plain": "0.84392563769995677"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 28, "cell_type": "code", "source": "from sklearn import metrics\nprint(metrics.classification_report(target, predicted,\n    target_names=['non-spam', 'spam']))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             precision    recall  f1-score   support\n\n   non-spam       0.84      1.00      0.91      1929\n       spam       1.00      0.06      0.11       384\n\navg / total       0.87      0.84      0.78      2313\n\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Sklearn SVM ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": "from sklearn import svm\nclf = svm.SVC(class_weight = 'auto', kernel = 'linear')\nclf.fit(X_train_counts_normalized, target)", "outputs": [{"execution_count": 29, "output_type": "execute_result", "data": {"text/plain": "SVC(C=1.0, cache_size=200, class_weight='auto', coef0=0.0, degree=3,\n  gamma=0.0, kernel='linear', max_iter=-1, probability=False,\n  random_state=None, shrinking=True, tol=0.001, verbose=False)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 30, "cell_type": "code", "source": "predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\nprint float(sum(predict)) / len(predict)\n\npredict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint 1- float(sum(predict)) / len(predict)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1.0\n0.986521513738\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 31, "cell_type": "code", "source": "predicted = clf.predict(count_vect_normalized.transform(testSpamNormalizedEmails + testNonSpamNormalizedEmails))\nprint(metrics.classification_report(testTarget, predicted,\n    target_names=['non-spam', 'spam']))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             precision    recall  f1-score   support\n\n   non-spam       1.00      0.98      0.99      2412\n       spam       0.92      0.98      0.95       481\n\navg / total       0.98      0.98      0.98      2893\n\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 32, "cell_type": "code", "source": "param_grid = [\n  {'C': [1, 10, 30, 60, 90, 120, 600, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 30, 60, 90, 120, 600, 1000], 'gamma': [0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n ]\n\nscores = ['precision', 'recall']", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 33, "cell_type": "code", "source": "from sklearn.grid_search import GridSearchCV\nfor score in scores:\n    clf = GridSearchCV(svm.SVC(), param_grid, cv=5, scoring=score)\n    clf.fit(X_train_counts_normalized, target)\n    \n    predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\n    print float(sum(predict)) / len(predict)\n    \n    predict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\n    print 1- float(sum(predict)) / len(predict)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.760416666667\n1.0\n0.963541666667\n1.0\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 34, "cell_type": "code", "source": "predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\nprint float(sum(predict)) / len(predict)\n\npredict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint 1- float(sum(predict)) / len(predict)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.963541666667\n1.0\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# My SVM Implementation  # Optimization Not Working", "cell_type": "markdown", "metadata": {}}, {"execution_count": 35, "cell_type": "code", "source": "def getMatrixFromNormalizedEmailList(emaiList, wordList):\n    t = []\n    for i in emaiList:\n        tmp = [ word in i for word in wordList ]  \n        t.append(tmp)\n    return sparse.csr_matrix(t)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 36, "cell_type": "code", "source": "trainingNonSpamVectors = getMatrixFromNormalizedEmailList(trainingNonSpamEmails, wordList)\ntrainingSpamVectors = getMatrixFromNormalizedEmailList(trainingSpamEmails, wordList)\ntestSpamVectors = getMatrixFromNormalizedEmailList(testSpamEmails, wordList)\ntestNonSpamVectors = getMatrixFromNormalizedEmailList(testNonSpamEmails, wordList)\n\n\n", "outputs": [], "metadata": {"scrolled": false, "collapsed": false, "trusted": true}}, {"execution_count": 37, "cell_type": "code", "source": "def getLinearKernelMatrix(trainingMatrix):\n    return trainingMatrix * trainingMatrix.T", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 40, "cell_type": "code", "source": "trainingMatrix = vstack([trainingSpamVectors, trainingNonSpamVectors])\ntrainingLabel = np.matrix([1 for i in trainingSpamVectors] + [-1 for i in trainingNonSpamVectors]).T\n\ntestgMatrix = vstack([testSpamVectors, testNonSpamVectors])\ntestLabel = np.matrix([1 for i in testSpamVectors] + [-1 for i in testNonSpamVectors]).T \n\ny = trainingLabel\n\nK = getLinearKernelMatrix(trainingMatrix)\n\nalpha0 = np.matrix([0 for i in trainingLabel]).T \n\nY = np.zeros((trainingLabel.shape[0], trainingLabel.shape[0]), int)\nfor i in range(0, Y.shape[0]):\n    for j in range(0, Y.shape[0]):\n        if i == j:\n            Y[i, j] = trainingLabel[i]", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "def jacobian(alpha):\n    return np.matrix([1 for i in trainingLabel]).T + Y * K * Y * alpha", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "def optimizeObj(alpha):\n    return (alpha.T * Y * K * Y.T * alpha)[0,0] + sum(alpha)\n\ndef constraint(alpha):\n    return (alpha.T * y)[0,0]\nprint constraint(y)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "cons = [{'type':'eq', 'fun': constraint}]", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "\nprint scipy.optimize.fmin_slsqp(optimizeObj, alpha0, f_ieqcons =constraint, fprime = jacobian)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "#Deciscion Tree ID3", "cell_type": "markdown", "metadata": {}}, {"execution_count": 164, "cell_type": "code", "source": "# use trainingMatrix.tolist() as traning vectors and y.tolist() as lables\nimport math\nimport operator\ndef calInformationCon(labels):\n    labelCounts = {}\n    totalCount = 0\n    for i in labels:\n        totalCount += 1\n        if i in labelCounts:\n            labelCounts[i] += 1\n        else:\n            labelCounts[i] = 1\n            \n    infoCont = 0.0\n    for key in labelCounts:\n        prob = float(labelCounts[key]) / totalCount\n        infoCont += -1 * prob * math.log(prob, 2)\n    return infoCont\n    \ndef splitDataByFeaturePosAndFeatureFilter(dataSet, labels, featurePos, f):\n    retDataSet = []\n    retLabel = []\n    i = 0\n    for row in dataSet:\n        if f(row[featurePos]):\n            reduced = row[:featurePos]\n            reduced.extend(row[featurePos + 1:])\n            retDataSet.append(reduced)\n            retLabel += [labels[i]]\n        i += 1\n    return retDataSet, retLabel\n        \ndef chooseBestFeatureIndexToSplit(dataSet, labels):\n    baseInfoContent = calInformationCon(labels)\n    bestFeature = -1\n    bestEntropy = 0\n    for i in range(len(dataSet[0])):\n        featureVals = [row[i] for row in dataSet]\n        featureSet = set(featureVals)\n        newEntropy = 0.0\n        for featureVal in featureSet:\n            splitData, splitLabels = splitDataByFeaturePosAndFeatureFilter(dataSet, labels, i, lambda x : x == featureVal)\n            prob = float(len(splitData)) / len(dataSet)\n            newEntropy += prob * calInformationCon(splitLabels)\n            if newEntropy > bestEntropy:\n                bestEntropy = newEntropy\n                bestFeature = i    \n    return i\n    \n    \ndef createTree(dataSet, labels):\n    if labels.count(labels[0]) == len(labels):\n        return lables[0]\n    if len(dataSet[0]) == 1:  # only one feature left, return majority Count\n        labelCount = {}\n        for i in distinctElements:\n            if i in labelCount:\n                labelCount[i] += 1\n            else:\n                labelCount[i] = 1\n                \n        sortedByValue = sorted(labelCount.iteritems(), key = operator.itemgetter(1), reverse=True)\n        return sortedByValue[0][0]\n\n    else:\n        splitFeatureIndex = chooseBestFeatureIndexToSplit(dataSet, labels)\n        id3Tree = {splitFeatureIndex:{}}\n        splitFeatureVals = set([ row[splitFeatureIndex] for row in dataSet])\n        for i in splitFeatureVals:\n            subData, subLabels = splitDataByFeaturePosAndFeatureFilter(dataSet, labels, splitFeatureIndex, lambda x: x == i)\n            id3Tree[splitFeatureIndex][i] = createTree(subData, subLabels)\n        return id3Tree\n        \n        ", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "dataSet = trainingMatrix.toarray().tolist()\nlabels = [ i[0] for i in y.tolist()]\ntree = createTree(dataSet, labels)\n\n#x = splitDataByFeaturePosAndFeatureFilter(dataSet, labels, 0, lambda x: x== False)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": false, "collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}