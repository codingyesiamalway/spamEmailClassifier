{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stemming.porter2 import stem\n",
    "import os, sys\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack, vstack\n",
    "lib_path = os.path.abspath(os.path.join('spamEmailClassifier'))\n",
    "sys.path.append(lib_path)\n",
    "from Preprocesser import * \n",
    "from NaiveBayes import *\n",
    "from svm import *\n",
    "\n",
    "# spamDir = '\"D:\\\\projects\\\\spamEmailClassifier\\\\spamDataset\"\n",
    "# nonspamDir = \"D:\\\\projects\\\\spamEmailClassifier\\\\nonspamDataset\"\n",
    "spamDir = 'spamDataset'\n",
    "nonspamDir = 'nonspamDataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingSetSpamFileList, testSetSpamFileList, trainingSetNonSpamFileList, testSetNonSpamFileList = getTrainingTestSet(spamDir, nonspamDir)\n",
    "trainingSpamTokenList, testSpamTokenList, trainingNonSpamTokenList, testNonSpamTokenList = getNormalizedEmailList(trainingSetSpamFileList, testSetSpamFileList, trainingSetNonSpamFileList, testSetNonSpamFileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272\n"
     ]
    }
   ],
   "source": [
    "wordList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability =  getSpamWordList(trainingSpamTokenList,trainingNonSpamTokenList, 4, 9, 3, 99)\n",
    "print len(wordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set spam:  0.934895833333\n",
      "training set non-spam:  0.995852773458\n",
      "training set overall:  0.986190944028\n",
      "\n",
      "test set spam:  0.939708939709\n",
      "test set non-spam:  0.995854063018\n",
      "test set overall:  0.986954918743\n"
     ]
    }
   ],
   "source": [
    "p,x,y = predict(trainingSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\n",
    "s =  sum(p) / float(len(p))\n",
    "print \"training set spam: \", s\n",
    "p,x,y = predict(trainingNonSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\n",
    "ns =  1 - sum(p) / float(len(p))\n",
    "print \"training set non-spam: \", ns\n",
    "print \"training set overall: \", s * spamProbability + ns * (1 - spamProbability)\n",
    "\n",
    "print \"\"\n",
    "p,x,y = predict(testSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\n",
    "s =  sum(p) / float(len(p))\n",
    "print \"test set spam: \", s\n",
    "p,x,y = predict(testNonSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\n",
    "ns =  1 - sum(p) / float(len(p))\n",
    "print \"test set non-spam: \", ns\n",
    "print \"test set overall: \", s * spamProbability + ns * (1 - spamProbability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, False]\n"
     ]
    }
   ],
   "source": [
    "myEmails = [\"I like ml\", \"here is a good offer to make millions dollars from littel investment\", \"what are you doing\"]\n",
    "x = map(lambda x : getTokensFromStr(x), myEmails)\n",
    "p,xs,yds = predict(x, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\n",
    "print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skkit-learn Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# def readFileListAndNormalize(fileList):\n",
    "#     res = []\n",
    "#     for i in fileList:\n",
    "#         with open(i, 'r') as file :\n",
    "#             data=file.read()\n",
    "#             res = res + [data]\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingSpamEmails = readFileListAndNormalize(trainingSetSpamFileList)\n",
    "testSpamEmails = readFileListAndNormalize(testSetSpamFileList)\n",
    "trainingNonSpamEmails = readFileListAndNormalize(trainingSetNonSpamFileList)\n",
    "testNonSpamEmails = readFileListAndNormalize(testSetNonSpamFileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(trainingSpamEmails + trainingNonSpamEmails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 49240)\n"
     ]
    }
   ],
   "source": [
    "print X_train_counts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313 2313\n"
     ]
    }
   ],
   "source": [
    "target = map (lambda x : 1, trainingSpamEmails)\n",
    "target += map(lambda x : 0, trainingNonSpamEmails)\n",
    "\n",
    "testTarget = map (lambda x : 1, testSpamEmails)\n",
    "testTarget += map(lambda x : 0, testNonSpamEmails)\n",
    "print len(target), len(trainingSpamEmails) + len(trainingNonSpamEmails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB().fit(X_train_tfidf, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict new emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_spam_count =  count_vect.transform(trainingSpamEmails)\n",
    "X_train_nonspam_count =  count_vect.transform(trainingNonSpamEmails)\n",
    "X_train_spam_tfidf =  tfidf_transformer.transform(X_train_spam_count)\n",
    "X_train_nonspam_tfidf =  tfidf_transformer.transform(X_train_nonspam_count)\n",
    "\n",
    "X_test_spam_count =  count_vect.transform(testSpamEmails)\n",
    "X_test_nonspam_count =  count_vect.transform(testNonSpamEmails)\n",
    "X_test_spam_tfidf =  tfidf_transformer.transform(X_test_spam_count)\n",
    "X_test_nonspam_tfidf =  tfidf_transformer.transform(X_test_nonspam_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training spam set:  0.611979166667\n",
      "training nonspam set:  0.00311041990669\n"
     ]
    }
   ],
   "source": [
    "X_train_spam_predicted = clf.predict(X_train_spam_tfidf) \n",
    "print \"training spam set: \", float(np.sum(X_train_spam_predicted)) / len(X_train_spam_predicted)\n",
    "\n",
    "X_train_nonspam_predicted = clf.predict(X_train_nonspam_tfidf) \n",
    "print \"training nonspam set: \", float(np.sum(X_train_nonspam_predicted)) / len(X_train_nonspam_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test spam set:  0.582120582121\n",
      "test nonspam set:  0.00248756218905\n"
     ]
    }
   ],
   "source": [
    "X_test_spam_predicted = clf.predict(X_test_spam_tfidf) \n",
    "print \"test spam set: \", float(np.sum(X_test_spam_predicted)) / len(X_test_spam_predicted)\n",
    "\n",
    "X_test_nonspam_predicted = clf.predict(X_test_nonspam_tfidf) \n",
    "print \"test nonspam set: \", float(np.sum(X_test_nonspam_predicted)) / len(X_test_nonspam_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of using tfidf,  my wordList generate better result.  SHOULD FIGURE OUT tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingSpamNormalizedEmails = map(lambda x : ' '.join(x), trainingSpamTokenList)\n",
    "testSpamNormalizedEmails = map(lambda x : ' '.join(x), testSpamTokenList)\n",
    "trainingNonSpamNormalizedEmails = map(lambda x : ' '.join(x), trainingNonSpamTokenList)\n",
    "testNonSpamNormalizedEmails = map(lambda x : ' '.join(x), testNonSpamTokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect_normalized = CountVectorizer()\n",
    "count_vect_normalized.vocabulary = wordList\n",
    "X_train_counts_normalized = count_vect_normalized.transform(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails)\n",
    "clf_normalized = BernoulliNB().fit(X_train_counts_normalized, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training spam set:  0.880208333333\n",
      "training nonspam set:  1.0\n",
      "test spam set:  0.866943866944\n",
      "test nonspam set:  0.999585406302\n"
     ]
    }
   ],
   "source": [
    "X_train_spam_normalized_predicted = clf_normalized.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails)) \n",
    "print \"training spam set: \", float(np.sum(X_train_spam_normalized_predicted)) / len(X_train_spam_normalized_predicted)\n",
    "\n",
    "X_train_nonspam_normalized__predicted = clf_normalized.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\n",
    "print \"training nonspam set: \", 1- float(np.sum(X_train_nonspam_normalized__predicted)) / len(X_train_nonspam_normalized__predicted)\n",
    "\n",
    "\n",
    "predict = clf_normalized.predict(count_vect_normalized.transform(testSpamNormalizedEmails)) \n",
    "print \"test spam set: \", float(np.sum(predict)) / len(predict)\n",
    "\n",
    "predict = clf_normalized.predict(count_vect_normalized.transform(testNonSpamNormalizedEmails))\n",
    "print \"test nonspam set: \", 1- float(np.sum(predict)) / len(predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf = text_clf.fit(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails, target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8430609597924773"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails)\n",
    "print predicted\n",
    "np.mean(predicted == target)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   non-spam       0.84      1.00      0.91      1929\n",
      "       spam       1.00      0.05      0.10       384\n",
      "\n",
      "avg / total       0.87      0.84      0.78      2313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(target, predicted,\n",
    "    target_names=['non-spam', 'spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='auto', coef0=0.0, degree=3,\n",
       "  gamma=0.0, kernel='linear', max_iter=-1, probability=False,\n",
       "  random_state=None, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(class_weight = 'auto', kernel = 'linear')\n",
    "clf.fit(X_train_counts_normalized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.987558320373\n"
     ]
    }
   ],
   "source": [
    "predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\n",
    "print float(sum(predict)) / len(predict)\n",
    "\n",
    "predict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\n",
    "print 1- float(sum(predict)) / len(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   non-spam       1.00      0.98      0.99      2412\n",
      "       spam       0.92      0.99      0.95       481\n",
      "\n",
      "avg / total       0.98      0.98      0.98      2893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(count_vect_normalized.transform(testSpamNormalizedEmails + testNonSpamNormalizedEmails))\n",
    "print(metrics.classification_report(testTarget, predicted,\n",
    "    target_names=['non-spam', 'spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 10, 30, 60, 90, 120, 600, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 30, 60, 90, 120, 600, 1000], 'gamma': [0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "scores = ['precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744791666667\n",
      "1.0\n",
      "0.9765625\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\metrics.py:1771: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "for score in scores:\n",
    "    clf = GridSearchCV(svm.SVC(), param_grid, cv=5, scoring=score)\n",
    "    clf.fit(X_train_counts_normalized, target)\n",
    "    \n",
    "    predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\n",
    "    print float(sum(predict)) / len(predict)\n",
    "    \n",
    "    predict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\n",
    "    print 1- float(sum(predict)) / len(predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9765625\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\n",
    "print float(sum(predict)) / len(predict)\n",
    "\n",
    "predict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\n",
    "print 1- float(sum(predict)) / len(predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My SVM Implementation  # Optimization Not Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMatrixFromNormalizedEmailList(emaiList, wordList):\n",
    "    t = []\n",
    "    for i in emaiList:\n",
    "        tmp = [ word in i for word in wordList ]  \n",
    "        t.append(tmp)\n",
    "    return sparse.csr_matrix(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainingNonSpamEmails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-675217b800db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainingNonSpamVectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetMatrixFromNormalizedEmailList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingNonSpamEmails\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrainingSpamVectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetMatrixFromNormalizedEmailList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSpamEmails\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtestSpamVectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetMatrixFromNormalizedEmailList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSpamEmails\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtestNonSpamVectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetMatrixFromNormalizedEmailList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestNonSpamEmails\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainingNonSpamEmails' is not defined"
     ]
    }
   ],
   "source": [
    "trainingNonSpamVectors = getMatrixFromNormalizedEmailList(trainingNonSpamEmails, wordList)\n",
    "trainingSpamVectors = getMatrixFromNormalizedEmailList(trainingSpamEmails, wordList)\n",
    "testSpamVectors = getMatrixFromNormalizedEmailList(testSpamEmails, wordList)\n",
    "testNonSpamVectors = getMatrixFromNormalizedEmailList(testNonSpamEmails, wordList)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLinearKernelMatrix(trainingMatrix):\n",
    "    return trainingMatrix * trainingMatrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingMatrix = vstack([trainingSpamVectors, trainingNonSpamVectors])\n",
    "trainingLabel = np.matrix([1 for i in trainingSpamVectors] + [-1 for i in trainingNonSpamVectors]).T\n",
    "\n",
    "testgMatrix = vstack([testSpamVectors, testNonSpamVectors])\n",
    "testLabel = np.matrix([1 for i in testSpamVectors] + [-1 for i in testNonSpamVectors]).T \n",
    "\n",
    "y = trainingLabel\n",
    "\n",
    "K = getLinearKernelMatrix(trainingMatrix)\n",
    "\n",
    "alpha0 = np.matrix([0 for i in trainingLabel]).T \n",
    "\n",
    "Y = np.zeros((trainingLabel.shape[0], trainingLabel.shape[0]), int)\n",
    "for i in range(0, Y.shape[0]):\n",
    "    for j in range(0, Y.shape[0]):\n",
    "        if i == j:\n",
    "            Y[i, j] = trainingLabel[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jacobian(alpha):\n",
    "    return np.matrix([1 for i in trainingLabel]).T + Y * K * Y * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimizeObj(alpha):\n",
    "    return (alpha.T * Y * K * Y.T * alpha)[0,0] + sum(alpha)\n",
    "\n",
    "def constraint(alpha):\n",
    "    return (alpha.T * y)[0,0]\n",
    "print constraint(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cons = [{'type':'eq', 'fun': constraint}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# print scipy.optimize.fmin_slsqp(optimizeObj, alpha0, f_ieqcons =constraint, fprime = jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deciscion Tree ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Tree ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability =  getSpamWordList(trainingSpamTokenList,trainingNonSpamTokenList, 30, 2, 2, 179)\n",
    "\n",
    "trainingSpamEmails = readFileListAndNormalize(trainingSetSpamFileList)\n",
    "testSpamEmails = readFileListAndNormalize(testSetSpamFileList)\n",
    "trainingNonSpamEmails = readFileListAndNormalize(trainingSetNonSpamFileList)\n",
    "testNonSpamEmails = readFileListAndNormalize(testSetNonSpamFileList)\n",
    "\n",
    "trainingNonSpamVectors = getMatrixFromNormalizedEmailList(trainingNonSpamEmails, wordList)\n",
    "trainingSpamVectors = getMatrixFromNormalizedEmailList(trainingSpamEmails, wordList)\n",
    "testSpamVectors = getMatrixFromNormalizedEmailList(testSpamEmails, wordList)\n",
    "testNonSpamVectors = getMatrixFromNormalizedEmailList(testNonSpamEmails, wordList)\n",
    "\n",
    "trainingMatrix = vstack([trainingSpamVectors, trainingNonSpamVectors])\n",
    "trainingLabel = np.matrix([1 for i in trainingSpamVectors] + [-1 for i in trainingNonSpamVectors]).T\n",
    "\n",
    "testgMatrix = vstack([testSpamVectors, testNonSpamVectors])\n",
    "testLabel = np.matrix([1 for i in testSpamVectors] + [-1 for i in testNonSpamVectors]).T \n",
    "\n",
    "y = trainingLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 (2313, 145) (2313L, 1L) (2893, 145)\n"
     ]
    }
   ],
   "source": [
    "print len(wordList), trainingMatrix.shape, trainingLabel.shape, testgMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingDataSet = trainingMatrix.toarray().tolist()\n",
    "trainingLabels = [ i[0] for i in trainingLabel.tolist()]\n",
    "testDataSet = testgMatrix.toarray().tolist()\n",
    "testLabels = [ i[0] for i in testLabel.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import ID3Tree as id3\n",
    "#id3 = reload(id3)\n",
    "\n",
    "# dump all useful data to file\n",
    "toDump = [trainingDataSet, trainingLabels, testDataSet, testLabels, wordList]\n",
    "fileNames = ['trainingDataSet', 'trainingLabels', 'testDataSet', 'testLabels', 'wordList_145']\n",
    "for i in range(len(toDump)):\n",
    "    id3.dumpToFile(toDump[i],fileNames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print len(testLabels)\n",
    "id3.dumpToFile(wordList,fileNames[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingDataSet = grabDataFromFile('trainingDataSet')\n",
    "trainingLabels = grabDataFromFile('trainingLabels')\n",
    "testDataSet = grabDataFromFile('testDataSet')\n",
    "testLabels = grabDataFromFile('testLabels')\n",
    "wordList = grabDataFromFile('wordList_145')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tree = id3.createTree(trainingDataSet, trainingLabels, wordList, len(wordList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictList = []\n",
    "for i in trainingDataSet:\n",
    "    t = id3.predict(i, tree, wordList)\n",
    "    if t is None:\n",
    "        predictList += [ '-9' ]\n",
    "    else:\n",
    "        predictList += [t ]\n",
    "trainingPred = [trainingLabels[i] == predictList[i] for i in range(len(predictList))]\n",
    "predictList = []\n",
    "\n",
    "for i in testDataSet:\n",
    "    t = id3.predict(i, tree, wordList)\n",
    "    if t is None:\n",
    "        predictList += [ '-9' ]\n",
    "    else:\n",
    "        predictList += [t ]\n",
    "testPred = [testLabels[i] == predictList[i] for i in range(len(predictList))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Training set accuracy: ', float(sum(trainingPred)) / len(trainingPred)\n",
    "print 'Test set accuracy: ', float(sum(testPred)) / len(testPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ID3Tree as id3\n",
    "id3 = reload(id3)\n",
    "\n",
    "# def createForrest(dataSet, labels, numTrees, bootstrapSampleSize, numFeatures):\n",
    "#     bootstrapDataSet = id3.bootstrapTrainingData(dataSet, labels, bootstrapSampleSize)\n",
    "#     trees = []\n",
    "#     for i in range(numTrees):\n",
    "#         tree = id3.createTree(bootstrapDataSet, labels, wordList, numFeatures)\n",
    "#         trees += [tree]\n",
    "#     return trees\n",
    "    \n",
    "# def forrestPredict(emailVector, trees, wordList):\n",
    "#     res = []\n",
    "#     for i in trees:\n",
    "#         res += [ id3.predict(emailVector, tree, wordList)  ]\n",
    "#     countSpam = len(  [i for i in res if i == 1] )\n",
    "#     countNonSpam = len(res) - countSpam\n",
    "#     if countSpam > countNonSpam:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees = id3.createForrest(trainingDataSet, trainingLabels, numTrees=20, bootstrapSampleSize=500, numFeatures=80, wordList = wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id3.dumpToFile(trees, 'forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.833981841764\n",
      "Test set accuracy:  0.8337366056\n"
     ]
    }
   ],
   "source": [
    "trainingPred = id3.getPredictionAccuracyUsingTrees(trees, trainingDataSet, trainingLabel, wordList)\n",
    "testPred = id3.getPredictionAccuracyUsingTrees(trees, testDataSet, testLabel, wordList)\n",
    "print 'Training set accuracy: ', trainingPred\n",
    "print 'Test set accuracy: ', testPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print testPred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
