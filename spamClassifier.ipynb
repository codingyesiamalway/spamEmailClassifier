{"nbformat_minor": 0, "cells": [{"execution_count": 12, "cell_type": "code", "source": "from stemming.porter2 import stem\nimport os, sys\nimport math\nimport numpy as np\nimport scipy\nfrom scipy import sparse\nfrom scipy.sparse import hstack, vstack\nlib_path = os.path.abspath(os.path.join('spamEmailClassifier'))\nsys.path.append(lib_path)\nfrom Preprocesser import * \nfrom NaiveBayes import *\nfrom svm import *\n\n# spamDir = '\"D:\\\\projects\\\\spamEmailClassifier\\\\spamDataset\"\n# nonspamDir = \"D:\\\\projects\\\\spamEmailClassifier\\\\nonspamDataset\"\nspamDir = 'spamDataset'\nnonspamDir = 'nonspamDataset'", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "# My Naive Bayes Classifier", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "trainingSetSpamFileList, testSetSpamFileList, trainingSetNonSpamFileList, testSetNonSpamFileList = getTrainingTestSet(spamDir, nonspamDir)\ntrainingSpamTokenList, testSpamTokenList, trainingNonSpamTokenList, testNonSpamTokenList = getNormalizedEmailList(trainingSetSpamFileList, testSetSpamFileList, trainingSetNonSpamFileList, testSetNonSpamFileList)", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 14, "cell_type": "code", "source": "wordList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability =  getSpamWordList(trainingSpamTokenList,trainingNonSpamTokenList, 4, 9, 3, 99)\nprint len(wordList)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1220\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "### Prediction ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "p,x,y = predict(trainingSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\ns =  sum(p) / float(len(p))\nprint \"training set spam: \", s\np,x,y = predict(trainingNonSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nns =  1 - sum(p) / float(len(p))\nprint \"training set non-spam: \", ns\nprint \"training set overall: \", s * spamProbability + ns * (1 - spamProbability)\n\nprint \"\"\np,x,y = predict(testSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\ns =  sum(p) / float(len(p))\nprint \"test set spam: \", s\np,x,y = predict(testNonSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nns =  1 - sum(p) / float(len(p))\nprint \"test set non-spam: \", ns\nprint \"test set overall: \", s * spamProbability + ns * (1 - spamProbability)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training set spam:  0.934895833333\ntraining set non-spam:  0.997407983411\ntraining set overall:  0.987726049309\n\ntest set spam:  0.933471933472\ntest set non-spam:  0.996268656716\ntest set overall:  0.986542647684\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 11, "cell_type": "code", "source": "myEmails = [\"I like ml\", \"here is a good offer to make millions dollars from littel investment\", \"what are you doing\"]\nx = map(lambda x : getTokensFromStr(x), myEmails)\np,xs,yds = predict(x, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nprint p", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[False, True, False]\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "# Skkit-learn Naive Bayes", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "from sklearn.feature_extraction.text import CountVectorizer\n# def readFileListAndNormalize(fileList):\n#     res = []\n#     for i in fileList:\n#         with open(i, 'r') as file :\n#             data=file.read()\n#             res = res + [data]\n#     return res", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 13, "cell_type": "code", "source": "trainingSpamEmails = readFileListAndNormalize(trainingSetSpamFileList)\ntestSpamEmails = readFileListAndNormalize(testSetSpamFileList)\ntrainingNonSpamEmails = readFileListAndNormalize(trainingSetNonSpamFileList)\ntestNonSpamEmails = readFileListAndNormalize(testSetNonSpamFileList)", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": 14, "cell_type": "code", "source": "count_vect = CountVectorizer()", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 15, "cell_type": "code", "source": "X_train_counts = count_vect.fit_transform(trainingSpamEmails + trainingNonSpamEmails)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 16, "cell_type": "code", "source": "print X_train_counts[0].shape", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(1, 49300)\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 17, "cell_type": "code", "source": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 18, "cell_type": "code", "source": "target = map (lambda x : 1, trainingSpamEmails)\ntarget += map(lambda x : 0, trainingNonSpamEmails)\n\ntestTarget = map (lambda x : 1, testSpamEmails)\ntestTarget += map(lambda x : 0, testNonSpamEmails)\nprint len(target), len(trainingSpamEmails) + len(trainingNonSpamEmails)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "2313 2313\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 19, "cell_type": "code", "source": "from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB().fit(X_train_tfidf, target)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "### predict new emails", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "X_train_spam_count =  count_vect.transform(trainingSpamEmails)\nX_train_nonspam_count =  count_vect.transform(trainingNonSpamEmails)\nX_train_spam_tfidf =  tfidf_transformer.transform(X_train_spam_count)\nX_train_nonspam_tfidf =  tfidf_transformer.transform(X_train_nonspam_count)\n\nX_test_spam_count =  count_vect.transform(testSpamEmails)\nX_test_nonspam_count =  count_vect.transform(testNonSpamEmails)\nX_test_spam_tfidf =  tfidf_transformer.transform(X_test_spam_count)\nX_test_nonspam_tfidf =  tfidf_transformer.transform(X_test_nonspam_count)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 21, "cell_type": "code", "source": "X_train_spam_predicted = clf.predict(X_train_spam_tfidf) \nprint \"training spam set: \", float(np.sum(X_train_spam_predicted)) / len(X_train_spam_predicted)\n\nX_train_nonspam_predicted = clf.predict(X_train_nonspam_tfidf) \nprint \"training nonspam set: \", float(np.sum(X_train_nonspam_predicted)) / len(X_train_nonspam_predicted)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training spam set:  0.619791666667\ntraining nonspam set:  0.00259201658891\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 22, "cell_type": "code", "source": "X_test_spam_predicted = clf.predict(X_test_spam_tfidf) \nprint \"test spam set: \", float(np.sum(X_test_spam_predicted)) / len(X_test_spam_predicted)\n\nX_test_nonspam_predicted = clf.predict(X_test_nonspam_tfidf) \nprint \"test nonspam set: \", float(np.sum(X_test_nonspam_predicted)) / len(X_test_nonspam_predicted)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "test spam set:  0.602910602911\ntest nonspam set:  0.00248756218905\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "### Instead of using tfidf,  my wordList generate better result.  SHOULD FIGURE OUT tfidf", "cell_type": "markdown", "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "trainingSpamNormalizedEmails = map(lambda x : ' '.join(x), trainingSpamTokenList)\ntestSpamNormalizedEmails = map(lambda x : ' '.join(x), testSpamTokenList)\ntrainingNonSpamNormalizedEmails = map(lambda x : ' '.join(x), trainingNonSpamTokenList)\ntestNonSpamNormalizedEmails = map(lambda x : ' '.join(x), testNonSpamTokenList)", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 24, "cell_type": "code", "source": "count_vect_normalized = CountVectorizer()\ncount_vect_normalized.vocabulary = wordList\nX_train_counts_normalized = count_vect_normalized.transform(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails)\nclf_normalized = BernoulliNB().fit(X_train_counts_normalized, target)\n\n", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 25, "cell_type": "code", "source": "X_train_spam_normalized_predicted = clf_normalized.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails)) \nprint \"training spam set: \", float(np.sum(X_train_spam_normalized_predicted)) / len(X_train_spam_normalized_predicted)\n\nX_train_nonspam_normalized__predicted = clf_normalized.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint \"training nonspam set: \", 1- float(np.sum(X_train_nonspam_normalized__predicted)) / len(X_train_nonspam_normalized__predicted)\n\n\npredict = clf_normalized.predict(count_vect_normalized.transform(testSpamNormalizedEmails)) \nprint \"test spam set: \", float(np.sum(predict)) / len(predict)\n\npredict = clf_normalized.predict(count_vect_normalized.transform(testNonSpamNormalizedEmails))\nprint \"test nonspam set: \", 1- float(np.sum(predict)) / len(predict)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training spam set:  0.908854166667\ntraining nonspam set:  0.998963193364\ntest spam set:  0.906444906445\ntest nonspam set:  0.999170812604\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "### use pipeline", "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "from sklearn.pipeline import Pipeline\ntext_clf = Pipeline([('vect', CountVectorizer()),\n     ('tfidf', TfidfTransformer()),\n     ('clf', MultinomialNB()),\n])\ntext_clf = text_clf.fit(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails, target)  ", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 27, "cell_type": "code", "source": "predicted = text_clf.predict(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails)\nprint predicted\nnp.mean(predicted == target)    ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[0 1 0 ..., 0 0 0]\n"}, {"execution_count": 27, "output_type": "execute_result", "data": {"text/plain": "0.84392563769995677"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 28, "cell_type": "code", "source": "from sklearn import metrics\nprint(metrics.classification_report(target, predicted,\n    target_names=['non-spam', 'spam']))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             precision    recall  f1-score   support\n\n   non-spam       0.84      1.00      0.91      1929\n       spam       1.00      0.06      0.11       384\n\navg / total       0.87      0.84      0.78      2313\n\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "# Sklearn SVM ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": "from sklearn import svm\nclf = svm.SVC(class_weight = 'auto', kernel = 'linear')\nclf.fit(X_train_counts_normalized, target)", "outputs": [{"execution_count": 29, "output_type": "execute_result", "data": {"text/plain": "SVC(C=1.0, cache_size=200, class_weight='auto', coef0=0.0, degree=3,\n  gamma=0.0, kernel='linear', max_iter=-1, probability=False,\n  random_state=None, shrinking=True, tol=0.001, verbose=False)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 30, "cell_type": "code", "source": "predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\nprint float(sum(predict)) / len(predict)\n\npredict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint 1- float(sum(predict)) / len(predict)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1.0\n0.986521513738\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 31, "cell_type": "code", "source": "predicted = clf.predict(count_vect_normalized.transform(testSpamNormalizedEmails + testNonSpamNormalizedEmails))\nprint(metrics.classification_report(testTarget, predicted,\n    target_names=['non-spam', 'spam']))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             precision    recall  f1-score   support\n\n   non-spam       1.00      0.98      0.99      2412\n       spam       0.92      0.98      0.95       481\n\navg / total       0.98      0.98      0.98      2893\n\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 32, "cell_type": "code", "source": "param_grid = [\n  {'C': [1, 10, 30, 60, 90, 120, 600, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 30, 60, 90, 120, 600, 1000], 'gamma': [0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n ]\n\nscores = ['precision', 'recall']", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 33, "cell_type": "code", "source": "from sklearn.grid_search import GridSearchCV\nfor score in scores:\n    clf = GridSearchCV(svm.SVC(), param_grid, cv=5, scoring=score)\n    clf.fit(X_train_counts_normalized, target)\n    \n    predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\n    print float(sum(predict)) / len(predict)\n    \n    predict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\n    print 1- float(sum(predict)) / len(predict)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.760416666667\n1.0\n0.963541666667\n1.0\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 34, "cell_type": "code", "source": "predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\nprint float(sum(predict)) / len(predict)\n\npredict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint 1- float(sum(predict)) / len(predict)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.963541666667\n1.0\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "# My SVM Implementation  # Optimization Not Working", "cell_type": "markdown", "metadata": {}}, {"execution_count": 35, "cell_type": "code", "source": "def getMatrixFromNormalizedEmailList(emaiList, wordList):\n    t = []\n    for i in emaiList:\n        tmp = [ word in i for word in wordList ]  \n        t.append(tmp)\n    return sparse.csr_matrix(t)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 36, "cell_type": "code", "source": "trainingNonSpamVectors = getMatrixFromNormalizedEmailList(trainingNonSpamEmails, wordList)\ntrainingSpamVectors = getMatrixFromNormalizedEmailList(trainingSpamEmails, wordList)\ntestSpamVectors = getMatrixFromNormalizedEmailList(testSpamEmails, wordList)\ntestNonSpamVectors = getMatrixFromNormalizedEmailList(testNonSpamEmails, wordList)\n\n\n", "outputs": [], "metadata": {"scrolled": false, "collapsed": false, "trusted": false}}, {"execution_count": 37, "cell_type": "code", "source": "def getLinearKernelMatrix(trainingMatrix):\n    return trainingMatrix * trainingMatrix.T", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": 40, "cell_type": "code", "source": "trainingMatrix = vstack([trainingSpamVectors, trainingNonSpamVectors])\ntrainingLabel = np.matrix([1 for i in trainingSpamVectors] + [-1 for i in trainingNonSpamVectors]).T\n\ntestgMatrix = vstack([testSpamVectors, testNonSpamVectors])\ntestLabel = np.matrix([1 for i in testSpamVectors] + [-1 for i in testNonSpamVectors]).T \n\ny = trainingLabel\n\nK = getLinearKernelMatrix(trainingMatrix)\n\nalpha0 = np.matrix([0 for i in trainingLabel]).T \n\nY = np.zeros((trainingLabel.shape[0], trainingLabel.shape[0]), int)\nfor i in range(0, Y.shape[0]):\n    for j in range(0, Y.shape[0]):\n        if i == j:\n            Y[i, j] = trainingLabel[i]", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "def jacobian(alpha):\n    return np.matrix([1 for i in trainingLabel]).T + Y * K * Y * alpha", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "def optimizeObj(alpha):\n    return (alpha.T * Y * K * Y.T * alpha)[0,0] + sum(alpha)\n\ndef constraint(alpha):\n    return (alpha.T * y)[0,0]\nprint constraint(y)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "cons = [{'type':'eq', 'fun': constraint}]", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "\nprint scipy.optimize.fmin_slsqp(optimizeObj, alpha0, f_ieqcons =constraint, fprime = jacobian)", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"source": "#Deciscion Tree ID3", "cell_type": "markdown", "metadata": {}}, {"source": "### get wordList for ID3", "cell_type": "markdown", "metadata": {}}, {"execution_count": 66, "cell_type": "code", "source": "wordList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability =  getSpamWordList(trainingSpamTokenList,trainingNonSpamTokenList, 30, 2, 2, 179)\n\ntrainingSpamEmails = readFileListAndNormalize(trainingSetSpamFileList)\ntestSpamEmails = readFileListAndNormalize(testSetSpamFileList)\ntrainingNonSpamEmails = readFileListAndNormalize(trainingSetNonSpamFileList)\ntestNonSpamEmails = readFileListAndNormalize(testSetNonSpamFileList)\n\ntrainingNonSpamVectors = getMatrixFromNormalizedEmailList(trainingNonSpamEmails, wordList)\ntrainingSpamVectors = getMatrixFromNormalizedEmailList(trainingSpamEmails, wordList)\ntestSpamVectors = getMatrixFromNormalizedEmailList(testSpamEmails, wordList)\ntestNonSpamVectors = getMatrixFromNormalizedEmailList(testNonSpamEmails, wordList)\n\ntrainingMatrix = vstack([trainingSpamVectors, trainingNonSpamVectors])\ntrainingLabel = np.matrix([1 for i in trainingSpamVectors] + [-1 for i in trainingNonSpamVectors]).T\n\ntestgMatrix = vstack([testSpamVectors, testNonSpamVectors])\ntestLabel = np.matrix([1 for i in testSpamVectors] + [-1 for i in testNonSpamVectors]).T \n\ny = trainingLabel", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 67, "cell_type": "code", "source": "print len(wordList), trainingMatrix.shape, trainingLabel.shape, testgMatrix.shape", "outputs": [{"output_type": "stream", "name": "stdout", "text": "139 (2313, 139) (2313L, 1L) (2893, 139)\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 114, "cell_type": "code", "source": "traningDataSet = trainingMatrix.toarray().tolist()\ntraningLabels = [ i[0] for i in trainingLabel.tolist()]\ntestDataSet = testgMatrix.toarray().tolist()\ntestLabels = [ i[0] for i in testLabel.tolist()]", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 122, "cell_type": "code", "source": "import ID3Tree\nID3Tree = reload(ID3Tree)\ntree = createTree(traningDataSet, traningLabels, wordList)", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 124, "cell_type": "code", "source": "#print tree", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 152, "cell_type": "code", "source": "predictList = []\nfor i in traningDataSet:\n    predictList += [ predict(i, tree, wordList)]\npred = [traningLabels[i] == predictList[i] for i in range(len(predictList))]", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 153, "cell_type": "code", "source": "print float(sum(pred)) / len(pred)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.16299178556\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.8", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}