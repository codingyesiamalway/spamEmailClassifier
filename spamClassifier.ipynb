{"nbformat_minor": 0, "cells": [{"execution_count": 214, "cell_type": "code", "source": "import os, sys\nimport math\nimport numpy as np\nimport scipy\nfrom scipy import sparse\nfrom scipy.sparse import hstack, vstack\nlib_path = os.path.abspath(os.path.join('D:\\projects\\spamEmailClassifier'))\nsys.path.append(lib_path)\nfrom Preprocesser import * \nfrom NaiveBayes import *\nfrom svm import *", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# My Naive Bayes Classifier", "cell_type": "markdown", "metadata": {}}, {"execution_count": 110, "cell_type": "code", "source": "trainingSetSpamFileList, testSetSpamFileList, trainingSetNonSpamFileList, testSetNonSpamFileList = getTrainingTestSet(\"D:\\\\projects\\\\spamEmailClassifier\\\\spamDataset\", \"D:\\\\projects\\\\spamEmailClassifier\\\\nonspamDataset\")\ntrainingSpamTokenList, testSpamTokenList, trainingNonSpamTokenList, testNonSpamTokenList = getNormalizedEmailList(trainingSetSpamFileList, testSetSpamFileList, trainingSetNonSpamFileList, testSetNonSpamFileList)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 111, "cell_type": "code", "source": "wordList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability =  getSpamWordList(trainingSpamTokenList,trainingNonSpamTokenList, 4, 9, 3, 99)\nprint len(wordList)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1292\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Prediction ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 112, "cell_type": "code", "source": "p,x,y = predict(trainingSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\ns =  sum(p) / float(len(p))\nprint \"training set spam: \", s\np,x,y = predict(trainingNonSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nns =  1 - sum(p) / float(len(p))\nprint \"training set non-spam: \", ns\nprint \"training set overall: \", s * spamProbability + ns * (1 - spamProbability)\n\nprint \"\"\np,x,y = predict(testSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\ns =  sum(p) / float(len(p))\nprint \"test set spam: \", s\np,x,y = predict(testNonSpamTokenList, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nns =  1 - sum(p) / float(len(p))\nprint \"test set non-spam: \", ns\nprint \"test set overall: \", s * spamProbability + ns * (1 - spamProbability)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training set spam:  0.924479166667\ntraining set non-spam:  0.996371176776\ntraining set overall:  0.985425457927\n\ntest set spam:  0.925155925156\ntest set non-spam:  0.99543946932\ntest set overall:  0.984738642998\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 113, "cell_type": "code", "source": "myEmails = [\"I like ml\", \"here is a good offer to make millions dollars from littel investment\", \"what are you doing\"]\nx = map(lambda x : getTokensFromStr(x), myEmails)\np,xs,yds = predict(x, wordDistributionInSpam, wordDistributionInNonSpam, spamProbability)\nprint p", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[False, True, False]\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Skkit-learn Naive Bayes", "cell_type": "markdown", "metadata": {}}, {"execution_count": 114, "cell_type": "code", "source": "from sklearn.feature_extraction.text import CountVectorizer\ndef readFileListAndNormalize(fileList):\n    res = []\n    for i in fileList:\n        with open(i, 'r') as file :\n            data=file.read()\n            res = res + [data]\n    return res", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 115, "cell_type": "code", "source": "trainingSpamEmails = readFileListAndNormalize(trainingSetSpamFileList)\ntestSpamEmails = readFileListAndNormalize(testSetSpamFileList)\ntrainingNonSpamEmails = readFileListAndNormalize(trainingSetNonSpamFileList)\ntestNonSpamEmails = readFileListAndNormalize(testSetNonSpamFileList)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 116, "cell_type": "code", "source": "count_vect = CountVectorizer()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 117, "cell_type": "code", "source": "X_train_counts = count_vect.fit_transform(trainingSpamEmails + trainingNonSpamEmails)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 118, "cell_type": "code", "source": "print X_train_counts[0].shape", "outputs": [{"output_type": "stream", "name": "stdout", "text": "(1, 48654)\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 119, "cell_type": "code", "source": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 120, "cell_type": "code", "source": "target = map (lambda x : 1, trainingSpamEmails)\ntarget += map(lambda x : 0, trainingNonSpamEmails)\n\ntestTarget = map (lambda x : 1, testSpamEmails)\ntestTarget += map(lambda x : 0, testNonSpamEmails)\nprint len(target), len(trainingSpamEmails) + len(trainingNonSpamEmails)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "2313 2313\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 121, "cell_type": "code", "source": "from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB().fit(X_train_tfidf, target)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### predict new emails", "cell_type": "markdown", "metadata": {}}, {"execution_count": 122, "cell_type": "code", "source": "X_train_spam_count =  count_vect.transform(trainingSpamEmails)\nX_train_nonspam_count =  count_vect.transform(trainingNonSpamEmails)\nX_train_spam_tfidf =  tfidf_transformer.transform(X_train_spam_count)\nX_train_nonspam_tfidf =  tfidf_transformer.transform(X_train_nonspam_count)\n\nX_test_spam_count =  count_vect.transform(testSpamEmails)\nX_test_nonspam_count =  count_vect.transform(testNonSpamEmails)\nX_test_spam_tfidf =  tfidf_transformer.transform(X_test_spam_count)\nX_test_nonspam_tfidf =  tfidf_transformer.transform(X_test_nonspam_count)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 123, "cell_type": "code", "source": "X_train_spam_predicted = clf.predict(X_train_spam_tfidf) \nprint \"training spam set: \", float(np.sum(X_train_spam_predicted)) / len(X_train_spam_predicted)\n\nX_train_nonspam_predicted = clf.predict(X_train_nonspam_tfidf) \nprint \"training nonspam set: \", float(np.sum(X_train_nonspam_predicted)) / len(X_train_nonspam_predicted)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training spam set:  0.609375\ntraining nonspam set:  0.00207361327112\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 124, "cell_type": "code", "source": "X_test_spam_predicted = clf.predict(X_test_spam_tfidf) \nprint \"test spam set: \", float(np.sum(X_test_spam_predicted)) / len(X_test_spam_predicted)\n\nX_test_nonspam_predicted = clf.predict(X_test_nonspam_tfidf) \nprint \"test nonspam set: \", float(np.sum(X_test_nonspam_predicted)) / len(X_test_nonspam_predicted)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "test spam set:  0.596673596674\ntest nonspam set:  0.0016583747927\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Instead of using tfidf,  my wordList generate better result.  SHOULD FIGURE OUT tfidf", "cell_type": "markdown", "metadata": {}}, {"execution_count": 125, "cell_type": "code", "source": "trainingSpamNormalizedEmails = map(lambda x : ' '.join(x), trainingSpamTokenList)\ntestSpamNormalizedEmails = map(lambda x : ' '.join(x), testSpamTokenList)\ntrainingNonSpamNormalizedEmails = map(lambda x : ' '.join(x), trainingNonSpamTokenList)\ntestNonSpamNormalizedEmails = map(lambda x : ' '.join(x), testNonSpamTokenList)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 126, "cell_type": "code", "source": "count_vect_normalized = CountVectorizer()\ncount_vect_normalized.vocabulary = wordList\nX_train_counts_normalized = count_vect_normalized.transform(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails)\nclf_normalized = BernoulliNB().fit(X_train_counts_normalized, target)\n\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 127, "cell_type": "code", "source": "X_train_spam_normalized_predicted = clf_normalized.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails)) \nprint \"training spam set: \", float(np.sum(X_train_spam_normalized_predicted)) / len(X_train_spam_normalized_predicted)\n\nX_train_nonspam_normalized__predicted = clf_normalized.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint \"training nonspam set: \", 1- float(np.sum(X_train_nonspam_normalized__predicted)) / len(X_train_nonspam_normalized__predicted)\n\n\npredict = clf_normalized.predict(count_vect_normalized.transform(testSpamNormalizedEmails)) \nprint \"test spam set: \", float(np.sum(predict)) / len(predict)\n\npredict = clf_normalized.predict(count_vect_normalized.transform(testNonSpamNormalizedEmails))\nprint \"test nonspam set: \", 1- float(np.sum(predict)) / len(predict)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "training spam set:  0.908854166667\ntraining nonspam set:  0.998963193364\ntest spam set:  0.906444906445\ntest nonspam set:  0.998756218905\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### use pipeline", "cell_type": "markdown", "metadata": {}}, {"execution_count": 128, "cell_type": "code", "source": "from sklearn.pipeline import Pipeline\ntext_clf = Pipeline([('vect', CountVectorizer()),\n     ('tfidf', TfidfTransformer()),\n     ('clf', MultinomialNB()),\n])\ntext_clf = text_clf.fit(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails, target)  ", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 129, "cell_type": "code", "source": "predicted = text_clf.predict(trainingSpamNormalizedEmails + trainingNonSpamNormalizedEmails)\nprint predicted\nnp.mean(predicted == target)    ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[0 0 0 ..., 0 0 0]\n"}, {"execution_count": 129, "output_type": "execute_result", "data": {"text/plain": "0.84392563769995677"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 130, "cell_type": "code", "source": "from sklearn import metrics\nprint(metrics.classification_report(target, predicted,\n    target_names=['non-spam', 'spam']))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             precision    recall  f1-score   support\n\n   non-spam       0.84      1.00      0.91      1929\n       spam       1.00      0.06      0.11       384\n\navg / total       0.87      0.84      0.78      2313\n\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Sklearn SVM ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 131, "cell_type": "code", "source": "from sklearn import svm\nclf = svm.SVC(class_weight = 'auto', kernel = 'linear')\nclf.fit(X_train_counts_normalized, target)", "outputs": [{"execution_count": 131, "output_type": "execute_result", "data": {"text/plain": "SVC(C=1.0, cache_size=200, class_weight='auto', coef0=0.0, degree=3,\n  gamma=0.0, kernel='linear', max_iter=-1, probability=False,\n  random_state=None, shrinking=True, tol=0.001, verbose=False)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 132, "cell_type": "code", "source": "predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\nprint float(sum(predict)) / len(predict)\n\npredict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint 1- float(sum(predict)) / len(predict)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1.0\n0.984966303784\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 133, "cell_type": "code", "source": "predicted = clf.predict(count_vect_normalized.transform(testSpamNormalizedEmails + testNonSpamNormalizedEmails))\nprint(metrics.classification_report(testTarget, predicted,\n    target_names=['non-spam', 'spam']))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             precision    recall  f1-score   support\n\n   non-spam       1.00      0.98      0.99      2412\n       spam       0.91      1.00      0.95       481\n\navg / total       0.98      0.98      0.98      2893\n\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 134, "cell_type": "code", "source": "param_grid = [\n  {'C': [1, 10, 30, 60, 90, 120, 600, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 30, 60, 90, 120, 600, 1000], 'gamma': [0.01, 0.001, 0.0001], 'kernel': ['rbf']},\n ]\n\nscores = ['precision', 'recall']", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 135, "cell_type": "code", "source": "from sklearn.grid_search import GridSearchCV\nfor score in scores:\n    clf = GridSearchCV(svm.SVC(), param_grid, cv=5, scoring=score)\n    clf.fit(X_train_counts_normalized, target)\n    \n    predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\n    print float(sum(predict)) / len(predict)\n    \n    predict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\n    print 1- float(sum(predict)) / len(predict)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.760416666667\n1.0\n0.96875\n0.999481596682\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 136, "cell_type": "code", "source": "predict = clf.predict(count_vect_normalized.transform(trainingSpamNormalizedEmails))\nprint float(sum(predict)) / len(predict)\n\npredict = clf.predict(count_vect_normalized.transform(trainingNonSpamNormalizedEmails))\nprint 1- float(sum(predict)) / len(predict)\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.96875\n0.999481596682\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# My SVM Implementation", "cell_type": "markdown", "metadata": {}}, {"execution_count": 154, "cell_type": "code", "source": "# def getMatrixFromNormalizedEmailList(emaiList, wordList):\n#     t = []\n#     for i in emaiList:\n#         tmp = [ word in i for word in wordList ]  \n#         t.append(tmp)\n#     return sparse.csr_matrix(t)\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 169, "cell_type": "code", "source": "trainingNonSpamVectors = getMatrixFromNormalizedEmailList(trainingNonSpamEmails, wordList)\ntrainingSpamVectors = getMatrixFromNormalizedEmailList(trainingSpamEmails, wordList)\ntestSpamVectors = getMatrixFromNormalizedEmailList(testSpamEmails, wordList)\ntestNonSpamVectors = getMatrixFromNormalizedEmailList(testNonSpamEmails, wordList)\n\n\n", "outputs": [], "metadata": {"scrolled": false, "collapsed": false, "trusted": true}}, {"execution_count": 193, "cell_type": "code", "source": "def getLinearKernelMatrix(trainingMatrix):\n    return trainingMatrix * trainingMatrix.T", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 263, "cell_type": "code", "source": "trainingMatrix = vstack([trainingSpamVectors, trainingNonSpamVectors])\ntrainingLabel = np.matrix([1 for i in trainingSpamVectors] + [-1 for i in trainingNonSpamVectors]).T\n\ntestgMatrix = vstack([testSpamVectors, testNonSpamVectors])\ntestLabel = np.matrix([1 for i in testSpamVectors] + [-1 for i in testNonSpamVectors]).T \n\ny = trainingLabel\n\nK = getLinearKernelMatrix(trainingMatrix)\n\nalpha0 = np.matrix([0 for i in trainingLabel]).T \n\nY = np.zeros((trainingLabel.shape[0], trainingLabel.shape[0]), int)\nfor i in range(0, a.shape[0]):\n    for j in range(0, a.shape[0]):\n        if i == j:\n            Y[i, j] = trainingLabel[i]", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 280, "cell_type": "code", "source": "def jacobian(alpha):\n    return np.matrix([1 for i in trainingLabel]).T + Y * K * Y * alpha", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 276, "cell_type": "code", "source": "def optimizeObj(alpha):\n    return (alpha.T * Y * K * Y.T * alpha)[0,0] + sum(alpha)\n\ndef constraint(alpha):\n    return (alpha.T * y)[0,0]\nprint constraint(y)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "2313\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 277, "cell_type": "code", "source": "cons = [{'type':'eq', 'fun': constraint}]", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 285, "cell_type": "code", "source": "\nprint scipy.optimize.fmin_slsqp(optimizeObj, alpha0, f_ieqcons =constraint, fprime = jacobian)", "outputs": [{"ename": "ValueError", "evalue": "all the input arrays must have same number of dimensions", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)", "\u001b[1;32m<ipython-input-285-99e497ca949e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmin_slsqp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizeObj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_ieqcons\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\slsqp.pyc\u001b[0m in \u001b[0;36mfmin_slsqp\u001b[1;34m(func, x0, eqcons, f_eqcons, ieqcons, f_ieqcons, bounds, fprime, fprime_eqcons, fprime_ieqcons, args, iter, acc, iprint, disp, full_output, epsilon, callback)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     res = _minimize_slsqp(func, x0, args, jac=fprime, bounds=bounds,\n\u001b[1;32m--> 206\u001b[1;33m                           constraints=cons, **opts)\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fun'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\scipy\\optimize\\slsqp.pyc\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, **unknown_options)\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[1;31m# Compute the derivatives of the objective function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;31m# For some reason SLSQP wants g dimensioned to n+1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfprime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;31m# Compute the normals of the constraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\lib\\function_base.pyc\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3883\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3884\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"], "output_type": "error"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.8", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}